<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://evercurse.github.io/</id>
    <title>Devops Art</title>
    <updated>2019-06-23T16:17:50.140Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://evercurse.github.io/"/>
    <link rel="self" href="https://evercurse.github.io//atom.xml"/>
    <subtitle>Cloud Native of Devops</subtitle>
    <logo>https://evercurse.github.io//images/avatar.png</logo>
    <icon>https://evercurse.github.io//favicon.ico</icon>
    <rights>All rights reserved 2019, Devops Art</rights>
    <entry>
        <title type="html"><![CDATA[2.Pod控制器]]></title>
        <id>https://evercurse.github.io//post/2pod-kong-zhi-qi</id>
        <link href="https://evercurse.github.io//post/2pod-kong-zhi-qi">
        </link>
        <updated>2019-06-23T16:17:36.000Z</updated>
        <content type="html"><![CDATA[<h4 id="常见pod控制器">常见pod控制器</h4>
<ul>
<li>Replication Controller</li>
<li>ReplicaSet</li>
<li>Deployment</li>
<li>DaemonSet</li>
<li>StatefulSet</li>
<li>Job</li>
<li>CronJob</li>
</ul>
<h4 id="控制器与pod对象-至少包含以下三个部分">控制器与pod对象 至少包含以下三个部分</h4>
<ul>
<li>标签选择器</li>
<li>期望副本数量</li>
<li>模板</li>
</ul>
<pre><code>apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: rs-example
spec:
  replicas:2
  selector:
    matchLabels: 
      app: rs-demo
  template: 
    metadata:
      labels:
        app:rs-demo 
    spec:
      containers:
      - name: myapp
        image: ikubernetes/myapp:v1 
        ports:
        - name: http
          containerPort: 80
</code></pre>
<h4 id="rs-rc">rs rc</h4>
<blockquote>
<p>Rs 支持滚动更新 以及强大的标签选择器 rc基本淘汰 rs也不常用 一般选择更为高级的deployment(基于rs)</p>
</blockquote>
<h3 id="rs控制器">rs控制器</h3>
<h4 id="更新rs控制器">更新rs控制器</h4>
<pre><code>修改yaml文件 例如image的版本
再执行kubectl apply 或者 kubectl replace
</code></pre>
<h4 id="rs扩容缩容">rs扩容缩容</h4>
<pre><code>kubectl scale replicasets rs-example --replicas=3
</code></pre>
<h4 id="删除rs控制器资源">删除rs控制器资源</h4>
<pre><code>kubectl delete replicasets rs-example -cascade=false #cascade=false这里是取消级联删除 与rs绑定的pod不会被删除
</code></pre>
<h3 id="deployment">Deployment</h3>
<h4 id="与rs相比的高级特性">与rs相比的高级特性</h4>
<ul>
<li>事件和状态查看  可查看应用升级的状态和进度</li>
<li>回滚</li>
<li>版本记录</li>
<li>暂停和启动</li>
<li>多种自动更新方案 1.recreate 完全删除重建 2.RollingUpdate(default)</li>
</ul>
<h4 id="创建deploy">创建deploy</h4>
<pre><code>apiVersion: apps/vl 
kind: Deployment 
metadata:
  name: myapp-deploy 
spec :
  replicas: 3 
  selector:
    matchLabels: 
      app: myapp
  template: 
    metadata: 
      labels:
        app: myapp 
    spec:
      containers:
      - name: myapp
        image: ikubernetes/myapp:vl
        ports:
        - containerPort: 80
          name: http
</code></pre>
<pre><code>kubectl apply -f myapp-deploy.yaml --record #创建
</code></pre>
<h4 id="deploy-更新">deploy 更新</h4>
<blockquote>
<p>滚动升级其实是新旧rs控制器交替减少或增加的过程 最终老的rs记录到历史记录中 但它所控制的pod不再受管控</p>
</blockquote>
<h4 id="滚动更新重要参数">滚动更新重要参数</h4>
<ul>
<li>
<p>maxSurge 指定升级期间存在的总Pod对象数量最多可超出期望值的个数 可以是百分比</p>
</li>
<li>
<p>maxUnavailable 升级期间正常可用的Pod副本数(包括新 旧版本)最多不能低于期望数值的个数  也可以是百分比 默认1</p>
</li>
</ul>
<blockquote>
<p>maxSurge 和 maxUnavailable 属性的值不可同时为 0</p>
</blockquote>
<h4 id="升级deploy">升级deploy</h4>
<pre><code>kubectl set image deployments myapp-deploy myapp=ikubernetes/myapp:v2
</code></pre>
<pre><code>kubectl rollout stacus deployments myapp-deploy 查看滚动更新状态
</code></pre>
<pre><code>kubectl get deployments myapp-deploy --watch 也可以查看到滚动更新pod变化的过程
</code></pre>
<h4 id="金丝雀发布">金丝雀发布</h4>
<pre><code>首先要设置maxSure为一个值 例如1和maxUnavailable=0(不影响老版本pod的删除) 相当于新增一个新版本pod 然后pause更新

kubectl set image deployments myapp-deploy myapp=ikubernetes/myapp:v3 \ &amp;&amp; kubectl rollout pause deployments myapp deploy

没问题的话恢复更新
kubectl rollout resume deployments myapp-deploy


</code></pre>
<h4 id="回滚">回滚</h4>
<blockquote>
<p>如有问题 速度回滚
kubectl rollout undo deployments myapp deploy #也可以加--to-revision回滚到指定的历史版本号</p>
<p>历史版本查看
kubectl rollout history deployments myapp-deploy</p>
<p>需要注意回滚操作也会记录到history中 被回滚的版本会从history中删除 初次之外 如果滚动更新处于暂停状态中 那么需要回滚之前现在pod版本修改为之前的状态 否则处于暂停状态无法回滚</p>
</blockquote>
<h4 id="扩容缩容">扩容缩容</h4>
<ul>
<li>修改yaml 然后kubectrl apply</li>
<li>kubectrl edit xxx</li>
<li>kubectl scale  类似rs的扩容</li>
</ul>
<h3 id="daemonset">DaemonSet</h3>
<h4 id="应用场景">应用场景</h4>
<ul>
<li>
<p>运行集群存储的守护进程，如在各个节点上运行 glusterd或 ceph。</p>
</li>
<li>
<p>在各个节点上运行日志收集守护进程，如 fluentd和 logstash。</p>
</li>
<li>
<p>在各个节点上运行监控系统的代理守护进程，如 Prometheus Node Exporter， collectd、 Datadog agent New Relic agent 或 Ganglia gmond 等</p>
</li>
</ul>
<h3 id="job控制器">job控制器</h3>
<blockquote>
<p>job完成后 控制器不会对该pod进行重启 而是标记为Complete状态 如job有失败那么根据配置来决定是否重启(restartPolicy)</p>
</blockquote>
<h4 id="job控制分类">job控制分类</h4>
<ul>
<li>单工作队列 串行 某一时刻只有一个pod</li>
<li>多工作队列 并行 可以设置工作队列数 每个队列仅负责一个作业</li>
</ul>
<h4 id="job应用场景">job应用场景</h4>
<ul>
<li>计算</li>
<li>备份</li>
</ul>
<p>创建job</p>
<pre><code>apiVersion: batch/v1
kind: Job
metadata:
  name: pi
spec:
  template:
    spec:
      containers:
      - name: pi
        image: perl
        command: [&quot;perl&quot;,  &quot;-Mbignum=bpi&quot;, &quot;-wle&quot;, &quot;print bpi(2000)&quot;]
      restartPolicy: Never #Never Always OnFailure 
</code></pre>
<h4 id="job-重要参数没有细看">job 重要参数(没有细看)</h4>
<ul>
<li>completions</li>
<li>parallelism</li>
<li>backoffLimit</li>
<li>activeDeadlineSeconds</li>
</ul>
<h4 id="job扩容-和deploy一样没有细看">job扩容 和deploy一样(没有细看)</h4>
<h3 id="cronjob">CronJob</h3>
<p><img src="https://s2.ax1x.com/2019/06/24/ZFb9UJ.png" alt="cronjob"></p>
<h4 id="cronjob重要参数">cronjob重要参数</h4>
<ul>
<li>scheduler 调度周期</li>
<li>concurrencyPolicy  并发策略 Allow Forbid Replace(替换)</li>
</ul>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[1.管理pod对象容器]]></title>
        <id>https://evercurse.github.io//post/1guan-li-pod-dui-xiang-rong-qi</id>
        <link href="https://evercurse.github.io//post/1guan-li-pod-dui-xiang-rong-qi">
        </link>
        <updated>2019-06-23T14:33:18.000Z</updated>
        <content type="html"><![CDATA[<h4 id="管理pod对象容器">管理pod对象容器</h4>
<blockquote>
<p>镜像拉取策略 imagePullPolicy</p>
</blockquote>
<pre><code>Always IfNotPresent Never
</code></pre>
<blockquote>
<p>pod port</p>
</blockquote>
<pre><code>apiV ersion: vl 
kind: Pod metadata:
name: pod-example 
spec:
  containers : 
  - name: myapp
    image : ikubernetes/myapp:vl
    ports:
    - name: http # 端口名
      containerPort: 80  #pod暴露的容器端口
      protocol: TCP # tcp/udp 默认tcp

</code></pre>
<blockquote>
<p>通过hostPort hostIP 暴露 pod  hostPort与 NodePort类型的 Service对象暴露端口的方式不同， NodePort
是通过所有节点暴露容器服务，而 hostPort则是经由 Pod对象所在节点的 IP地址来进行。</p>
</blockquote>
<pre><code> - name: http
   containerPort: 80
   hostIP: 0.0.0.0
   hostPort: 89
   protocol: TCP
</code></pre>
<blockquote>
<p>向pod对象容器传递环境参数</p>
</blockquote>
<ul>
<li>
<p>env</p>
<pre><code>env:
- name: REDIS_HOST
  value: db.ilinux.io:6379 
- name: LOG_LEVEL
  value: info
</code></pre>
</li>
<li>
<p>envFrom (ConfigMap 和 Secret)</p>
</li>
</ul>
<blockquote>
<p>共享worker的网络名称空间 不推荐 如果deployment控制此类pod 每个node只能启动一个pod 因为会有端口冲突 在 Pod对象中时还可以分别使用 spec.hostPID 和 spec.hostIPC来共享工作节点的
PID和 IPC名称空间。</p>
</blockquote>
<pre><code>hostNetwork : true
</code></pre>
<blockquote>
<p>设置pod安全上下文 spec.securityContext</p>
</blockquote>
<pre><code>securityContext:
  runAsNonRoot: true
  runAsUser: 1000 
  allowPrivilegeEscalation: false
</code></pre>
<h4 id="标签以及标签选择器">标签以及标签选择器</h4>
<blockquote>
<p>为pod设置标签 以便于分类控制 事实上所有资源都能进行设置标签</p>
</blockquote>
<pre><code>metadata:
  name: pod-with-labels
  labels:
    env: qa
    tier: frontend
</code></pre>
<pre><code>kubectl get pod --show-labels 查看标签
</code></pre>
<pre><code>kubectl get pods -L env, tier 显示指定的标签
</code></pre>
<blockquote>
<p>给pod打标签(metadata 上定义) --overwrite 用于覆盖老的kv</p>
</blockquote>
<pre><code>kubectl label pods/pod-example env=production
</code></pre>
<blockquote>
<p>标签筛选 -l 筛选 = == != 三种等值筛选 -L 指定展示env标签</p>
</blockquote>
<pre><code>kubectl get pods -l &quot;env!=qa&quot; -L env
kubectl get pods -l &quot;env!=qa,tier=frontend&quot; -L env, tier
in notin exists
kubectl get pods -l &quot;env in (production,dev)&quot; -L env
列出标签键名env的值为production或dev，且不存在键名为tier的标签的所有 Pod 对象(单引号避免shell转义):
kubectl get pods -l 'env in (production,dev),!tier' -L env,tier
</code></pre>
<blockquote>
<p>标签选择器 其余资源可以通过标签来筛选pod (matchLabels matchExpressions)</p>
</blockquote>
<pre><code>selector: 
  matchLabels:
    component: redis #直接匹配
  matchExpressions: # 表达式筛选
    - {key: tier, operator: In, values: [cache]}
    - {key: environment, operator: Exists, values: }
</code></pre>
<blockquote>
<p>给node打标签</p>
</blockquote>
<pre><code>kubectl label nodes node01 disktype=ssd
kubectl get node -l &quot;disk&quot; -L &quot;disk&quot;

pod 引用(spec):
nodeSelector:
  disktype: ssd
  
pod直接指定node调度 spec.nodeName宇段直接指定目标节点
</code></pre>
<h4 id="资源注解">资源注解</h4>
<blockquote>
<p>注解 annotation</p>
</blockquote>
<pre><code>注解也是kv结构 但不受字符长度限制 一般用于描述该资源
metadata.annotations
</code></pre>
<h4 id="pod-对象的生命周期">Pod 对象的生命周期</h4>
<blockquote>
<p>pod相位</p>
</blockquote>
<ul>
<li>Pending  已经存入etcd 暂未调度</li>
<li>Running  pod已经被调度 kubelet完成所有容器</li>
<li>Succeeded  Pod中的所有容器都已经成功终止并且不会被重启</li>
<li>Failed   所有容器都已经终止 但至少有一个容器终止失败 即容器返回了非0值的退出状态或已经被系统终止</li>
<li>Unknown  API Server 无法正常获取到Pod对象的状态信息 通常是由于其无法与所在工作节点的 kubelet通信所致</li>
</ul>
<blockquote>
<p>pod 创建过程</p>
</blockquote>
<p><img src="https://s2.ax1x.com/2019/06/21/VzBOpD.png" alt="image-20190621120944194"></p>
<p><img src="https://s2.ax1x.com/2019/06/21/VzzLee.png" alt="image-20190621120944194"></p>
<blockquote>
<p>生命周期的行文</p>
</blockquote>
<ul>
<li>init container</li>
<li>postStart 但无法保证和ENTRYPOINT的顺序</li>
<li>preStop 会阻塞删除容器</li>
</ul>
<blockquote>
<p>生命周期钩子处理器</p>
</blockquote>
<ul>
<li>
<p>exec</p>
</li>
<li>
<p>http</p>
<pre><code>lifecycle:
  postStart:
    exec:
      command: [&quot;/bin/sh&quot;, &quot;-c&quot;,&quot;echo 'lifecycle hooks handler' &gt; /usr/share/nginx/html/test.html&quot;]
</code></pre>
</li>
</ul>
<blockquote>
<p>容器探测 以下探测方式都有三种结果 Success Failure Unknown</p>
</blockquote>
<ul>
<li>ExecAction</li>
<li>TCPSocketAction</li>
<li>HTTPGetAction</li>
</ul>
<blockquote>
<p>kubelet 会在活动容器中执行两种类型的检测</p>
</blockquote>
<ul>
<li>livenessProbe 存活检测 未定义存活检测的就是success</li>
<li>readinessProbe 就绪检测</li>
</ul>
<blockquote>
<p>容器重启策略 restartPolicy</p>
</blockquote>
<ul>
<li>Always 一旦pod对象终止就重启 默认</li>
<li>OnFailure 仅在pod对象出现错误才会重启</li>
<li>Never 从不重启</li>
</ul>
<blockquote>
<p>pod停止流程</p>
</blockquote>
<pre><code>用户提交请求 对每一个容器发送kill信号并配置一个删除宽限期 如超过宽限期 则强制删除
默认情况下 ，所有删除操作的宽限期都是 30秒，不过kubectl delete命令可以使用&quot;--grace-period=&lt;seconds&gt;&quot;选项自定义其时长，若使用 0值则表示直接强制删除指定的资源.不过,此时需要同时为命令使用&quot;--force&quot;选项 
</code></pre>
<p><img src="https://s2.ax1x.com/2019/06/21/Vzvv0H.png" alt="image-20190621120944194"></p>
<h4 id="pod-存活性探测">Pod 存活性探测</h4>
<ul>
<li>
<p>exec 命令状态返回值</p>
<pre><code>livenessProbe:
  exec:
    command: [&quot;test&quot;,&quot;-e&quot;,&quot;/tmp/healthy&quot;]
</code></pre>
</li>
<li>
<p>http</p>
<pre><code>livenessProbe:
  httpGet:
    path: /healthz 
    port: http 
    scheme: HTTP
    #host 主机地址 默认pod的ip
    #httpHeaders 自定义请求头
</code></pre>
</li>
<li>
<p>tcp</p>
<pre><code>ports:
- name: http
  containerPort: 80
  livenessProbe:
    tcpSocket: 
      #host 默认pod ip
      port: http
</code></pre>
</li>
</ul>
<blockquote>
<p>存活检测的额外属性配置</p>
</blockquote>
<ul>
<li>initialDelaySeconds <int> 容器启动多久后开始探测 默认0s</li>
<li>timeoutSeconds <int> 存活检测超时时长 默认1s 最小也是1s</li>
<li>periodSeconds <int> 存活检测频率 默认10s 最小1s</li>
<li>successThreshold <int> 处于失败状态时 探测操作至少连续多少次的成功才被认为是通过检测 显示#success 属性，默认值为 1，最小值也为 1。</li>
<li>failureThreshold <int> 处于成功状态时 探测操作至少连续多少次的失败才被视为是检测不通过 显示为#failure 属性，默认值为 3， 最小值为 1。</li>
</ul>
<h4 id="pod就绪探测">pod就绪探测</h4>
<blockquote>
<p>和存活探测概念一致 目的用于等待程序完全启动成功 避免pod running 状态 但pod里面的服务未启动导致的请求失败问题</p>
</blockquote>
<h4 id="资源需求及资源限制">资源需求及资源限制</h4>
<p>####资源需求</p>
<blockquote>
<p>在 Kubernetes 系统上运行关键型业务相关的 Pod 时必须使用 requests 属性为容器定义资源的确保可用量</p>
</blockquote>
<pre><code>resources:
  requests:
    memory: &quot;128M&quot;  
    cpu: &quot;200m&quot; # 200/1000=0.2核
</code></pre>
<h4 id="资源限制">资源限制</h4>
<blockquote>
<p>多次重复重启会触发 <strong>Kubernetes 系统的重启延迟机制</strong>，即每次重启的时间间隔会不断地拉长 。 于是，用户看到的 Pod 资源的相关状态通常为&quot;CrashLoopBackOff&quot;</p>
</blockquote>
<pre><code>resources:
  requests:
    memory: &quot;64Mi&quot;
    cpu: &quot;1&quot;
  limits:
    memory: &quot;64Mi&quot; 
    cpu : &quot;1&quot;
</code></pre>
<p>####Pod 的服务质量类别</p>
<ul>
<li>Guaranteed 所有容器cpu和内存都设置了相同的request和limit</li>
<li>Burstable 至少有一个设置了 但不满足guaranteed</li>
<li>BestEffort 任何容器都未设置 优先级最低</li>
</ul>
<blockquote>
<p>同等质量类别 内存占用的比例大(与自身request对比)的优先被kill 与cpu无关</p>
</blockquote>
<h4 id="namespace">namespace</h4>
<ul>
<li>
<p>UTS: 主机名</p>
</li>
<li>
<p>IPC: 进程间通信</p>
</li>
<li>
<p>PID: &quot;chroot&quot;进程树</p>
</li>
<li>
<p>NS: 挂载点，首次登陆Linux</p>
</li>
<li>
<p>NET: 网络访问，包括接口</p>
</li>
<li>
<p>USER: 将本地的虚拟user-id映射到真实的user-id</p>
</li>
</ul>
]]></content>
    </entry>
</feed>